{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/johncalab/Dropbox/gitstuff/deepShowerThoughts/ai\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "csv_path = os.path.join('training_data', 'may15nov17_above130_less100.csv')\n",
    "print(os.getcwd())\n",
    "df = pd.read_csv(csv_path)\n",
    "trunc = df[df.score > 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class charVocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token \n",
    "                                for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "        self.mask_token = '<mask>'\n",
    "        self.begin_token = '<begin>'\n",
    "        self.end_token = '<end>'\n",
    "        self.unk_token = '<unk>'\n",
    "        self.space_token = ' '\n",
    "\n",
    "        self.mask_idx = self.add_token(self.mask_token)\n",
    "        self.begin_idx = self.add_token(self.begin_token)\n",
    "        self.end_idx = self.add_token(self.end_token)\n",
    "        self.unk_idx = self.add_token(self.unk_token)\n",
    "        self.space_idx = self.add_token(self.space_token)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.token_to_idx) == len(self.idx_to_token)\n",
    "        return len(self.token_to_idx)\n",
    "\n",
    "    def lookup_token(self,token):\n",
    "        return self.token_to_idx[token]\n",
    "\n",
    "    def lookup_idx(self,i):\n",
    "        return self.idx_to_token[i]\n",
    "\n",
    "    def add_txt(self,path):\n",
    "        with open(path, 'r') as f:\n",
    "            fulltext = f.read()\n",
    "            for c in fulltext:\n",
    "                if c != '\\n':\n",
    "                    self.add_token(c)\n",
    "        return None\n",
    "\n",
    "    def add_series(self,df):\n",
    "        for sentence in df:\n",
    "            max_len = min(300, len(sentence))\n",
    "            for char in sentence[:max_len]:\n",
    "                self.add_token(char)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = charVocabulary()\n",
    "vocab.add_series(trunc.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class charVectorizer(object):\n",
    "    def __init__(self,vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def vectorize(self, sent, max_len=-1):\n",
    "        \"\"\"\n",
    "        max_len is used to know how much to pad\n",
    "        \"\"\"\n",
    "        ind = [self.vocab.begin_idx]\n",
    "        ind.extend(self.vocab.lookup_token(token) for token in sent)\n",
    "        ind.append(self.vocab.end_idx)\n",
    "        \n",
    "        max_len = max(len(ind), max_len) + 1\n",
    "\n",
    "        x = np.empty(max_len-1, dtype=np.int64)\n",
    "        x[:len(ind)-1] = ind[:-1]\n",
    "        x[len(ind)-1:] = self.vocab.mask_idx\n",
    "\n",
    "        y = np.empty(max_len-1, dtype=np.int64)\n",
    "        y[:len(ind)-1] = ind[1:]\n",
    "        y[len(ind)-1:] = self.vocab.mask_idx\n",
    "\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = charVectorizer(vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  7,  4, 12, 17, 22, 13,  4, 35, 17, 22, 17, 22, 17, 18,  0,  0]),\n",
       " array([ 7,  4, 12, 17, 22, 13,  4, 35, 17, 22, 17, 22, 17, 18,  2,  0,  0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vectorize('i want bananas', max_len=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,_ = vectorizer.vectorize('i like', max_len=30)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class charDataset(Dataset):\n",
    "    def __init__(self,vectorizer,posts):\n",
    "        self.posts = posts\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "        max_len = len(posts.iloc[0])\n",
    "        for sentence in posts:\n",
    "            max_len = max(max_len, len(sentence))\n",
    "\n",
    "        self.max_len = max_len + 20\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        sent = self.posts.iloc[i]\n",
    "        x,y = self.vectorizer.vectorize(sent=sent, max_len=self.max_len)\n",
    "        assert x.shape == y.shape\n",
    "        assert x.shape[0] == self.max_len\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAKE = True\n",
    "class fakeDS(Dataset):\n",
    "    def __init__(self,vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.max_len = 32\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 512\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        x,y = self.vectorizer.vectorize(sent='hello.', max_len=8)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = fakeDS(vectorizer)\n",
    "posts = trunc.title\n",
    "ds = charDataset(vectorizer=vectorizer,posts=posts)\n",
    "dl = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "\n",
    "if FAKE:\n",
    "    ds = fakeDS(vectorizer=vectorizer)\n",
    "    dl = DataLoader(fake_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ds)):\n",
    "    try:\n",
    "        ds.__getitem__(i)\n",
    "    except:\n",
    "        print(i)\n",
    "\n",
    "x,y = ds.__getitem__(0)\n",
    "l = x.shape[0]\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "params = {}\n",
    "params['max_len'] = ds.max_len\n",
    "params['num_emb'] = len(vocab)\n",
    "params['emb_dim'] = 512\n",
    "params['mask_id'] = vocab.mask_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer weights\n",
    "It's ok that the Linear layer takes in a tensor with three indices.\n",
    "The point is Linear has in_features * out_features weights.\n",
    "It just reshapes the tensor and works with the last dimension, so it's all good.\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html#linear\n",
    "\n",
    "https://stackoverflow.com/questions/54444630/application-of-nn-linear-layer-in-pytorch-on-additional-dimentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_mech(nn.Module):\n",
    "    \"\"\"\n",
    "    Single attention mechanism using dot products.\n",
    "    \"\"\"\n",
    "    def __init__(self,emb_dim,mask_id,bias=False):\n",
    "        super(attention_mech,self).__init__()\n",
    "\n",
    "        self.query = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=bias)\n",
    "        self.key = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=bias)\n",
    "        self.value = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=bias)\n",
    "    \n",
    "    def forward(self,x,verbose=False):\n",
    "        b,s,d = x.size()\n",
    "        if verbose:\n",
    "            print(x.shape)\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "            \n",
    "        raw_weights = torch.bmm(q, k.transpose(1,2))\n",
    "        if verbose:\n",
    "            print(raw_weights.shape)\n",
    "        _,m,n = raw_weights.size()\n",
    "        indices = torch.triu_indices(m,n, offset=1)\n",
    "        raw_weights[:, indices[0], indices[1]] = float('-inf')\n",
    "        weights = F.softmax(raw_weights, dim=2)\n",
    "        out = torch.bmm(weights, v)\n",
    "        if verbose:\n",
    "            print(x1.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention(nn.Module):\n",
    "    \"\"\"\n",
    "    heads = number of simultaneous attention mechanisms\n",
    "    Run heads attention mechanisms and combine them together\n",
    "    \"\"\"\n",
    "    def __init__(self,emb_dim,mask_id,heads=8,bias=False):\n",
    "        super(self_attention,self).__init__()\n",
    "        \n",
    "        self.layers = [attention_mech(emb_dim=emb_dim,mask_id=mask_id,bias=bias) for _ in range(heads)]\n",
    "        self.W = nn.Linear(in_features=heads*emb_dim,out_features=emb_dim,bias=bias)\n",
    "        \n",
    "    def forward(self,x,verbose=False):\n",
    "        b,s,d = x.size()\n",
    "        if verbose:\n",
    "            print(x.shape)\n",
    "        \n",
    "#         x = torch.cat([layer(x) for layer in self.layers],dim=2)\n",
    "        # I included a relu before regrouping\n",
    "        x = torch.cat([F.relu(layer(x)) for layer in self.layers],dim=2)\n",
    "        if verbose:\n",
    "            print(x.shape)\n",
    "        \n",
    "        x = self.W(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine the self_attention block with a ff_multiple feed forward layers\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, mask_id,heads=8,bias=False,ff_multiple=4,dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = self_attention(emb_dim=emb_dim,mask_id=mask_id,heads=heads,bias=bias)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "                               nn.Linear(emb_dim,ff_multiple*emb_dim),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(ff_multiple*emb_dim,emb_dim))\n",
    "        \n",
    "        self.dout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # apply attention\n",
    "        attended = self.attention(x)\n",
    "        # normalize 1 + residual connection\n",
    "        normalized = self.norm1(attended + x)\n",
    "        # apply dropout\n",
    "        dropped = self.dout(normalized)\n",
    "        # look up MLP and feedforward\n",
    "        forwarded = self.ff(dropped)\n",
    "        # normalize 2 + residual connection\n",
    "        normalized_again = self.norm2(forwarded + dropped)\n",
    "        out = self.dout(normalized_again)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mini_transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    depth = number of transformer blocks to use\n",
    "    The architecture stacks depth TransformerBlock\n",
    "    \"\"\"\n",
    "    def __init__(self,num_emb,emb_dim,max_len,mask_id,heads=8,ff_multiple=4,depth=3,bias=False,dropout=0.1):\n",
    "        super(mini_transformer,self).__init__()\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=num_emb, embedding_dim=emb_dim)#, padding_idx=mask_id)\n",
    "        self.pos_emb = nn.Embedding(num_embeddings=max_len,embedding_dim=emb_dim)\n",
    "        \n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            t = TransformerBlock(emb_dim=emb_dim,mask_id=mask_id,heads=heads,bias=bias,ff_multiple=ff_multiple,dropout=dropout)\n",
    "            tblocks.append(t)\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=emb_dim, out_features=num_emb)\n",
    "    \n",
    "    def forward(self,x_in,verbose=False):\n",
    "        x = self.emb(x_in)\n",
    "        b,s,d = x.size()\n",
    "\n",
    "        positions = torch.arange(s)\n",
    "        positions = self.pos_emb(positions)\n",
    "        positions = positions[None, :, :]\n",
    "        positions = positions.expand(b, s, d)\n",
    "\n",
    "        x = x + positions\n",
    "        x1 = self.tblocks(x)\n",
    "        x2 = x1.contiguous().view(b*s, -1)\n",
    "        x3 = self.fc(x2)\n",
    "        out = x3.view(b,s,-1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class mini_transformer(nn.Module):\n",
    "#     def __init__(self,num_emb,emb_dim,max_len,mask_id,heads=8):\n",
    "#         super(mini_transformer,self).__init__()\n",
    "        \n",
    "#         self.max_len = max_len\n",
    "        \n",
    "#         self.emb = nn.Embedding(num_embeddings=num_emb, embedding_dim=emb_dim)#, padding_idx=mask_id)\n",
    "#         self.pos_emb = nn.Embedding(num_embeddings=max_len,embedding_dim=emb_dim)\n",
    "        \n",
    "#         self.Att = self_attention(emb_dim=emb_dim,mask_id=mask_id,heads=8)\n",
    "\n",
    "# #         self.am1 = attention_mech(emb_dim=emb_dim,max_len=max_len,mask_id=mask_id)\n",
    "# #         self.am2 = attention_mech(emb_dim=emb_dim,max_len=max_len,mask_id=mask_id)\n",
    "\n",
    "#         self.fc = nn.Linear(in_features=emb_dim, out_features=num_emb)\n",
    "    \n",
    "#     def forward(self,x_in,verbose=False):\n",
    "#         x = self.emb(x_in)\n",
    "#         b,s,d = x.size()\n",
    "\n",
    "#         positions = torch.arange(s)\n",
    "#         positions = self.pos_emb(positions)\n",
    "#         positions = positions[None, :, :]\n",
    "#         positions = positions.expand(b, s, d)\n",
    "\n",
    "#         x = x + positions\n",
    "\n",
    "#         x1 = self.Att(x)\n",
    "\n",
    "#         x2 = x1.contiguous().view(b*s, -1)\n",
    "#         x3 = self.fc(x2)\n",
    "#         out = x3.view(b,s,-1)\n",
    "#         if verbose:\n",
    "#             print(out.shape)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class silly(nn.Module):\n",
    "    def __init__(self,num_emb,emb_dim,max_len,mask_id):\n",
    "        super(silly,self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=num_emb, embedding_dim=emb_dim)#, padding_idx=mask_id)\n",
    "        self.lin1 = nn.Linear(in_features=emb_dim, out_features=emb_dim)\n",
    "        self.lu = nn.ReLU()\n",
    "        self.fc = nn.Linear(in_features=emb_dim, out_features=num_emb)\n",
    "    \n",
    "    def forward(self,x_in,verbose=False):\n",
    "        x = self.emb(x_in)\n",
    "        b,s,d = x.size()\n",
    "        x = x.contiguous().view(b*s, -1)\n",
    "        x = self.lin1(x)\n",
    "        x = self.lu(x)\n",
    "        x = self.fc(x)\n",
    "        out = x.view(b,s,-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(vocab,vectors):\n",
    "    b,s,d = vectors.size()\n",
    "    assert d == len(vocab)\n",
    "    x = vectors[0]\n",
    "    probs = F.softmax(x, dim=1)\n",
    "    sent = ''\n",
    "    for i in range(s):\n",
    "        v = probs[i,:]\n",
    "        # replace with argmax?\n",
    "        win = torch.multinomial(v, num_samples=1)\n",
    "        idx = win.item()\n",
    "        sent += vocab.lookup_idx(idx)\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mini_transformer(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.539181709289551 epoch 1\n",
      "1.4837013483047485 epoch 1\n",
      "0.9214379191398621 epoch 1\n",
      "0.45531606674194336 epoch 1\n",
      "0.19739538431167603 epoch 1\n",
      "0.07349017262458801 epoch 1\n",
      "0.049179017543792725 epoch 1\n",
      "0.046350665390491486 epoch 1\n",
      "0.023609647527337074 epoch 1\n",
      "0.0163254514336586 epoch 1\n",
      "0.01080833375453949 epoch 1\n",
      "0.007186246104538441 epoch 1\n",
      "0.005286970641463995 epoch 2\n",
      "0.003825703402981162 epoch 2\n",
      "0.0028278289828449488 epoch 2\n",
      "0.002434421330690384 epoch 2\n",
      "0.002116689458489418 epoch 2\n",
      "0.0017204589676111937 epoch 2\n",
      "0.0016445510555058718 epoch 2\n",
      "0.0014569866470992565 epoch 2\n",
      "0.0012268263380974531 epoch 2\n",
      "0.0012215226888656616 epoch 2\n",
      "0.0010451601119711995 epoch 2\n",
      "0.0009982752380892634 epoch 2\n",
      "0.0008707694360055029 epoch 2\n",
      "0.0008384878747165203 epoch 2\n",
      "0.000774865155108273 epoch 2\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "num_epochs = 2\n",
    "device = 'cpu'\n",
    "from tqdm import tqdm\n",
    "bestloss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    ### train ----\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for data in dl:\n",
    "        x,y = data\n",
    "        x.to(device)\n",
    "        y.to(device)\n",
    "        y_pred = model(x)\n",
    "        b,s,d = y_pred.shape\n",
    "        y_pred_to_loss = y_pred.view(b*s,d)\n",
    "        y_to_loss = y.view(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(y_pred_to_loss, y_to_loss)#, ignore_index=mask_id)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss.item() < bestloss:\n",
    "            bestloss = loss.item()\n",
    "            print(loss.item(), f\"epoch {epoch+1}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_idx(ind):\n",
    "    s = ''\n",
    "    for idx in ind:\n",
    "        if idx == 0:\n",
    "            break\n",
    "        s += vocab.lookup_idx(idx)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_samp(model,vocab,sample_size=30,prompt=\"\"):\n",
    "    ind = [vocab.begin_idx]\n",
    "    ind.extend([vocab.lookup_token(char) for char in prompt])\n",
    "    ind.extend([vocab.mask_idx for _ in range(len(prompt), model.max_len - 1)]) # plus or minus 1...\n",
    "    assert model.max_len == len(ind)\n",
    "\n",
    "    for i in range(len(prompt), sample_size):\n",
    "        x = torch.tensor(ind).unsqueeze(dim=0)\n",
    "        pred = model(x)\n",
    "        \n",
    "        b,s,d = pred.size()\n",
    "        assert d == len(vocab)\n",
    "        z = pred[0,i,:] # plus or minus one?\n",
    "        prob = F.softmax(z,dim=0)\n",
    "        win = torch.multinomial(prob, num_samples=1)\n",
    "        ind[i+1] = win.item()\n",
    "        \n",
    "    return decode_idx(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<begin>hello.<end>llo.<end>lo.<end>l<end>\n",
      "<begin>hello.<end>\n",
      "<begin>hello.<end>\n",
      "<begin>hello.<end>\n",
      "<begin>hello.<end>\n",
      "<begin>hello.<end>\n",
      "<begin>hello.<end>\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    print(gen_samp(model,vocab,prompt=''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
