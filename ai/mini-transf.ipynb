{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('may15nov17_above130_less100.csv')\n",
    "trunc = df[df.score > 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class charVocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token \n",
    "                                for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "        self.mask_token = '<mask>'\n",
    "        self.begin_token = '<begin>'\n",
    "        self.end_token = '<end>'\n",
    "        self.unk_token = '<unk>'\n",
    "        self.space_token = ' '\n",
    "\n",
    "        self.mask_idx = self.add_token(self.mask_token)\n",
    "        self.begin_idx = self.add_token(self.begin_token)\n",
    "        self.end_idx = self.add_token(self.end_token)\n",
    "        self.unk_idx = self.add_token(self.unk_token)\n",
    "        self.space_idx = self.add_token(self.space_token)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.token_to_idx) == len(self.idx_to_token)\n",
    "        return len(self.token_to_idx)\n",
    "\n",
    "    def lookup_token(self,token):\n",
    "        return self.token_to_idx[token]\n",
    "\n",
    "    def lookup_idx(self,i):\n",
    "        return self.idx_to_token[i]\n",
    "\n",
    "    def add_txt(self,path):\n",
    "        with open(path, 'r') as f:\n",
    "            fulltext = f.read()\n",
    "            for c in fulltext:\n",
    "                if c != '\\n':\n",
    "                    self.add_token(c)\n",
    "        return None\n",
    "\n",
    "    def add_series(self,df):\n",
    "        for sentence in df:\n",
    "            max_len = min(300, len(sentence))\n",
    "            for char in sentence[:max_len]:\n",
    "                self.add_token(char)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = charVocabulary()\n",
    "vocab.add_series(trunc.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class charVectorizer(object):\n",
    "    def __init__(self,vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def vectorize(self, sent, max_len=-1):\n",
    "        \"\"\"\n",
    "        max_len is used to know how much to pad\n",
    "        \"\"\"\n",
    "        ind = [self.vocab.begin_idx]\n",
    "        ind.extend(self.vocab.lookup_token(token) for token in sent)\n",
    "        ind.append(self.vocab.end_idx)\n",
    "        \n",
    "        max_len = max(len(ind), max_len) + 1\n",
    "\n",
    "        x = np.empty(max_len-1, dtype=np.int64)\n",
    "        x[:len(ind)-1] = ind[:-1]\n",
    "        x[len(ind)-1:] = self.vocab.mask_idx\n",
    "\n",
    "        y = np.empty(max_len-1, dtype=np.int64)\n",
    "        y[:len(ind)-1] = ind[1:]\n",
    "        y[len(ind)-1:] = self.vocab.mask_idx\n",
    "\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = charVectorizer(vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  7,  4, 12, 17, 22, 13,  4, 35, 17, 22, 17, 22, 17, 18,  0,  0]),\n",
       " array([ 7,  4, 12, 17, 22, 13,  4, 35, 17, 22, 17, 22, 17, 18,  2,  0,  0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vectorize('i want bananas', max_len=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,_ = vectorizer.vectorize('i like', max_len=30)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class charDataset(Dataset):\n",
    "    def __init__(self,vectorizer,posts):\n",
    "        self.posts = posts\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "        max_len = len(posts.iloc[0])\n",
    "        for sentence in posts:\n",
    "            max_len = max(max_len, len(sentence))\n",
    "\n",
    "        self.max_len = max_len + 20\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        sent = self.posts.iloc[i]\n",
    "        x,y = self.vectorizer.vectorize(sent=sent, max_len=self.max_len)\n",
    "        assert x.shape == y.shape\n",
    "        assert x.shape[0] == self.max_len\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fakeDS(Dataset):\n",
    "    def __init__(self,vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.max_len = 8\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 512\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        x,y = self.vectorizer.vectorize(sent='hello.', max_len=8)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = fakeDS(vectorizer)\n",
    "posts = trunc.title\n",
    "ds = charDataset(vectorizer=vectorizer,posts=posts)\n",
    "dl = DataLoader(ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120,)\n",
      "(120,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ds)):\n",
    "    try:\n",
    "        ds.__getitem__(i)\n",
    "    except:\n",
    "        print(i)\n",
    "\n",
    "x,y = ds.__getitem__(0)\n",
    "l = x.shape[0]\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "params = {}\n",
    "params['max_len'] = ds.max_len\n",
    "params['num_emb'] = len(vocab)\n",
    "params['emb_dim'] = 128\n",
    "params['mask_id'] = vocab.mask_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mini_transformer(nn.Module):\n",
    "    def __init__(self,num_emb,emb_dim,max_len,mask_id):\n",
    "        super(mini_transformer,self).__init__()\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=num_emb, embedding_dim=emb_dim)#, padding_idx=mask_id)\n",
    "        self.pos_emb = nn.Embedding(num_embeddings=max_len,embedding_dim=emb_dim)\n",
    "        self.query = nn.Linear(in_features=emb_dim, out_features=emb_dim)\n",
    "        self.key = nn.Linear(in_features=emb_dim, out_features=emb_dim)\n",
    "        self.value = nn.Linear(in_features=emb_dim, out_features=emb_dim)\n",
    "        self.fc = nn.Linear(in_features=emb_dim, out_features=num_emb)\n",
    "    \n",
    "    def forward(self,x_in,verbose=False):\n",
    "        x = self.emb(x_in)\n",
    "        b,s,d = x.size()\n",
    "\n",
    "        positions = torch.arange(s)\n",
    "        positions = self.pos_emb(positions)\n",
    "        positions = positions[None, :, :]\n",
    "        positions = positions.expand(b, s, d)\n",
    "\n",
    "        x = x + positions\n",
    "        \n",
    "        if verbose:\n",
    "            print(x_in.shape)\n",
    "        # each row is a vector of size emb_dim\n",
    "        if verbose:\n",
    "            print(x.shape)\n",
    "            \n",
    "        q = self.query(x)\n",
    "        k = self.query(x)\n",
    "        v = self.query(x)\n",
    "            \n",
    "        raw_weights = torch.bmm(q, k.transpose(1,2))\n",
    "        if verbose:\n",
    "            print(raw_weights.shape)\n",
    "        _,m,n = raw_weights.size()\n",
    "        indices = torch.triu_indices(m,n, offset=1)\n",
    "        raw_weights[:, indices[0], indices[1]] = float('-inf')\n",
    "        weights = F.softmax(raw_weights, dim=2)\n",
    "        x1 = torch.bmm(weights, v)\n",
    "        if verbose:\n",
    "            print(x1.shape)\n",
    "\n",
    "        x2 = x1.contiguous().view(b*s, -1)\n",
    "        x3 = self.fc(x2)\n",
    "        out = x3.view(b,s,-1)\n",
    "        if verbose:\n",
    "            print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class silly(nn.Module):\n",
    "    def __init__(self,num_emb,emb_dim,max_len,mask_id):\n",
    "        super(silly,self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=num_emb, embedding_dim=emb_dim)#, padding_idx=mask_id)\n",
    "        self.lin1 = nn.Linear(in_features=emb_dim, out_features=emb_dim)\n",
    "        self.lu = nn.ReLU()\n",
    "        self.fc = nn.Linear(in_features=emb_dim, out_features=num_emb)\n",
    "    \n",
    "    def forward(self,x_in,verbose=False):\n",
    "        x = self.emb(x_in)\n",
    "        b,s,d = x.size()\n",
    "        x = x.contiguous().view(b*s, -1)\n",
    "        x = self.lin1(x)\n",
    "        x = self.lu(x)\n",
    "        x = self.fc(x)\n",
    "        out = x.view(b,s,-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(vocab,vectors):\n",
    "    b,s,d = vectors.size()\n",
    "    assert d == len(vocab)\n",
    "    x = vectors[0]\n",
    "    probs = F.softmax(x, dim=1)\n",
    "    sent = ''\n",
    "    for i in range(s):\n",
    "        v = probs[i,:]\n",
    "        # replace with argmax?\n",
    "        win = torch.multinomial(v, num_samples=1)\n",
    "        idx = win.item()\n",
    "        sent += vocab.lookup_idx(idx)\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mini_transformer(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.717811107635498 epoch 1\n",
      "4.483091831207275 epoch 1\n",
      "4.156666278839111 epoch 1\n",
      "3.993055582046509 epoch 1\n",
      "3.7387778759002686 epoch 1\n",
      "3.417160987854004 epoch 1\n",
      "3.297903299331665 epoch 1\n",
      "3.0389068126678467 epoch 1\n",
      "2.9301400184631348 epoch 1\n",
      "2.8643951416015625 epoch 1\n",
      "2.8088467121124268 epoch 1\n",
      "2.4485177993774414 epoch 1\n",
      "2.439161777496338 epoch 1\n",
      "2.363636016845703 epoch 1\n",
      "2.347338914871216 epoch 1\n",
      "2.2844488620758057 epoch 1\n",
      "2.101871967315674 epoch 1\n",
      "2.0506784915924072 epoch 1\n",
      "1.9568417072296143 epoch 1\n",
      "1.9176318645477295 epoch 1\n",
      "1.8144317865371704 epoch 2\n",
      "1.8043575286865234 epoch 2\n",
      "1.70589017868042 epoch 2\n",
      "1.6552937030792236 epoch 2\n",
      "1.5934199094772339 epoch 2\n",
      "1.437622308731079 epoch 2\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "num_epochs = 2\n",
    "device = 'cpu'\n",
    "from tqdm import tqdm\n",
    "bestloss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    ### train ----\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for data in dl:\n",
    "        x,y = data\n",
    "        x.to(device)\n",
    "        y.to(device)\n",
    "        y_pred = model(x)\n",
    "        b,s,d = y_pred.shape\n",
    "        y_pred_to_loss = y_pred.view(b*s,d)\n",
    "        y_to_loss = y.view(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(y_pred_to_loss, y_to_loss)#, ignore_index=mask_id)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss.item() < bestloss:\n",
    "            bestloss = loss.item()\n",
    "            print(loss.item(), f\"epoch {epoch+1}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Idtk stsSu irohuh  tifcsbseyg😂bsny eeo ebci it  d e,tgus<unk>nreaarirotkolNoocgtla d#vohde h<end><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask><mask>'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_vector(vocab,model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "W<mask>\n"
     ]
    }
   ],
   "source": [
    "gen_samp(model,vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_idx(ind):\n",
    "    s = ''\n",
    "    for idx in ind:\n",
    "        if idx == 0:\n",
    "            break\n",
    "        s += vocab.lookup_idx(idx)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_samp(model,vocab,sample_size=30,prompt=\"\"):\n",
    "    ind = [vocab.begin_idx]\n",
    "    ind.extend([vocab.lookup_token(char) for char in prompt])\n",
    "    ind.extend([vocab.mask_idx for _ in range(len(prompt), model.max_len - 1)]) # plus or minus 1...\n",
    "    assert model.max_len == len(ind)\n",
    "\n",
    "    for i in range(len(prompt), sample_size):\n",
    "        x = torch.tensor(ind).unsqueeze(dim=0)\n",
    "        pred = model(x)\n",
    "        \n",
    "        b,s,d = pred.size()\n",
    "        assert d == len(vocab)\n",
    "        z = pred[0,i,:] # plus or minus one?\n",
    "        prob = F.softmax(z,dim=0)\n",
    "        win = torch.multinomial(prob, num_samples=1)\n",
    "        ind[i+1] = win.item()\n",
    "        \n",
    "    return decode_idx(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<begin>vaillor b0 trac wof atal i ofe\n"
     ]
    }
   ],
   "source": [
    "print(gen_samp(model,vocab,prompt='vai'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "bos = [vocab.begin_idx]\n",
    "# ind = bos.extend([vocab.lookup_token(char) for char in prompt])\n",
    "print(bos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2]\n",
    "b = [3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.extend(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
