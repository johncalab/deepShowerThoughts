{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.peterbloem.nl/blog/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/johncalab/Dropbox/gitstuff/deepShowerThoughts/ai\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "csv_path = os.path.join('training_data', 'may15nov17_above130_less100.csv')\n",
    "print(os.getcwd())\n",
    "df = pd.read_csv(csv_path)\n",
    "trunc = df[df.score > 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class charVocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token \n",
    "                                for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "        self.mask_token = '<mask>'\n",
    "        self.begin_token = '<begin>'\n",
    "        self.end_token = '<end>'\n",
    "        self.unk_token = '<unk>'\n",
    "        self.space_token = ' '\n",
    "\n",
    "        self.mask_idx = self.add_token(self.mask_token)\n",
    "        self.begin_idx = self.add_token(self.begin_token)\n",
    "        self.end_idx = self.add_token(self.end_token)\n",
    "        self.unk_idx = self.add_token(self.unk_token)\n",
    "        self.space_idx = self.add_token(self.space_token)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.token_to_idx) == len(self.idx_to_token)\n",
    "        return len(self.token_to_idx)\n",
    "\n",
    "    def lookup_token(self,token):\n",
    "        return self.token_to_idx[token]\n",
    "\n",
    "    def lookup_idx(self,i):\n",
    "        return self.idx_to_token[i]\n",
    "\n",
    "    def add_txt(self,path):\n",
    "        with open(path, 'r') as f:\n",
    "            fulltext = f.read()\n",
    "            for c in fulltext:\n",
    "                if c != '\\n':\n",
    "                    self.add_token(c)\n",
    "        return None\n",
    "\n",
    "    def add_series(self,df):\n",
    "        for sentence in df:\n",
    "            max_len = min(300, len(sentence))\n",
    "            for char in sentence[:max_len]:\n",
    "                self.add_token(char)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = charVocabulary()\n",
    "vocab.add_series(trunc.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class charVectorizer(object):\n",
    "    def __init__(self,vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def vectorize(self, sent, max_len=-1):\n",
    "        \"\"\"\n",
    "        max_len is used to know how much to pad\n",
    "        \"\"\"\n",
    "        ind = [self.vocab.begin_idx]\n",
    "        ind.extend(self.vocab.lookup_token(token) for token in sent)\n",
    "        ind.append(self.vocab.end_idx)\n",
    "        \n",
    "        max_len = max(len(ind), max_len) + 1\n",
    "\n",
    "        x = np.empty(max_len-1, dtype=np.int64)\n",
    "        x[:len(ind)-1] = ind[:-1]\n",
    "        x[len(ind)-1:] = self.vocab.mask_idx\n",
    "\n",
    "        y = np.empty(max_len-1, dtype=np.int64)\n",
    "        y[:len(ind)-1] = ind[1:]\n",
    "        y[len(ind)-1:] = self.vocab.mask_idx\n",
    "\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = charVectorizer(vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  7,  4, 12, 17, 22, 13,  4, 35, 17, 22, 17, 22, 17, 18,  0,  0]),\n",
       " array([ 7,  4, 12, 17, 22, 13,  4, 35, 17, 22, 17, 22, 17, 18,  2,  0,  0]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vectorize('i want bananas', max_len=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,_ = vectorizer.vectorize('i like', max_len=30)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class charDataset(Dataset):\n",
    "    def __init__(self,vectorizer,posts):\n",
    "        self.posts = posts\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "        max_len = len(posts.iloc[0])\n",
    "        for sentence in posts:\n",
    "            max_len = max(max_len, len(sentence))\n",
    "\n",
    "        self.max_len = max_len + 20\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        sent = self.posts.iloc[i]\n",
    "        x,y = self.vectorizer.vectorize(sent=sent, max_len=self.max_len)\n",
    "        assert x.shape == y.shape\n",
    "        assert x.shape[0] == self.max_len\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fakeDS(Dataset):\n",
    "    def __init__(self,vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.max_len = 8\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 512\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        x,y = self.vectorizer.vectorize(sent='hello.', max_len=8)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = fakeDS(vectorizer)\n",
    "posts = trunc.title\n",
    "ds = charDataset(vectorizer=vectorizer,posts=posts)\n",
    "dl = DataLoader(ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120,)\n",
      "(120,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ds)):\n",
    "    try:\n",
    "        ds.__getitem__(i)\n",
    "    except:\n",
    "        print(i)\n",
    "\n",
    "x,y = ds.__getitem__(0)\n",
    "l = x.shape[0]\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "params = {}\n",
    "params['max_len'] = ds.max_len\n",
    "params['num_emb'] = len(vocab)\n",
    "params['emb_dim'] = 128\n",
    "params['mask_id'] = vocab.mask_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer weights\n",
    "It's ok that the Linear layer takes in a tensor with three indices.\n",
    "The point is Linear has in_features * out_features weights.\n",
    "It just reshapes the tensor and works with the last dimension, so it's all good.\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html#linear\n",
    "\n",
    "https://stackoverflow.com/questions/54444630/application-of-nn-linear-layer-in-pytorch-on-additional-dimentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_mech(nn.Module):\n",
    "    def __init__(self,emb_dim,max_len,mask_id,bias=False):\n",
    "        super(attention_mech,self).__init__()\n",
    "        \n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.query = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=bias)\n",
    "        self.key = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=bias)\n",
    "        self.value = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=bias)\n",
    "    \n",
    "    def forward(self,x,verbose=False):\n",
    "        b,s,d = x.size()\n",
    "        if verbose:\n",
    "            print(x.shape)\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "            \n",
    "        raw_weights = torch.bmm(q, k.transpose(1,2))\n",
    "        if verbose:\n",
    "            print(raw_weights.shape)\n",
    "        _,m,n = raw_weights.size()\n",
    "        indices = torch.triu_indices(m,n, offset=1)\n",
    "        raw_weights[:, indices[0], indices[1]] = float('-inf')\n",
    "        weights = F.softmax(raw_weights, dim=2)\n",
    "        out = torch.bmm(weights, v)\n",
    "        if verbose:\n",
    "            print(x1.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dot_attention(nn.Module):\n",
    "    def __init__(self,emb_dim,max_len,mask_id,bias=False):\n",
    "        super(dot_attention,self).__init__()\n",
    "        \n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.query = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=bias)\n",
    "        self.key = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=bias)\n",
    "        self.value = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=bias)\n",
    "    \n",
    "    def forward(self,x,verbose=False):\n",
    "        b,s,d = x.size()\n",
    "        if verbose:\n",
    "            print(x.shape)\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "            \n",
    "        raw_weights = torch.bmm(q, k.transpose(1,2))\n",
    "        if verbose:\n",
    "            print(raw_weights.shape)\n",
    "        _,m,n = raw_weights.size()\n",
    "        indices = torch.triu_indices(m,n, offset=1)\n",
    "        raw_weights[:, indices[0], indices[1]] = float('-inf')\n",
    "        weights = F.softmax(raw_weights, dim=2)\n",
    "        out = torch.bmm(weights, v)\n",
    "        if verbose:\n",
    "            print(x1.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mini_transformer(nn.Module):\n",
    "    def __init__(self,num_emb,emb_dim,max_len,mask_id):\n",
    "        super(mini_transformer,self).__init__()\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=num_emb, embedding_dim=emb_dim)#, padding_idx=mask_id)\n",
    "        self.pos_emb = nn.Embedding(num_embeddings=max_len,embedding_dim=emb_dim)\n",
    "\n",
    "        self.am1 = attention_mech(emb_dim=emb_dim,max_len=max_len,mask_id=mask_id)\n",
    "        self.am2 = attention_mech(emb_dim=emb_dim,max_len=max_len,mask_id=mask_id)\n",
    "\n",
    "        self.fc = nn.Linear(in_features=emb_dim, out_features=num_emb)\n",
    "    \n",
    "    def forward(self,x_in,verbose=False):\n",
    "        x = self.emb(x_in)\n",
    "        b,s,d = x.size()\n",
    "\n",
    "        positions = torch.arange(s)\n",
    "        positions = self.pos_emb(positions)\n",
    "        positions = positions[None, :, :]\n",
    "        positions = positions.expand(b, s, d)\n",
    "\n",
    "        x = x + positions\n",
    "\n",
    "        x1 = self.am1(x) + self.am2(x)\n",
    "\n",
    "        x2 = x1.contiguous().view(b*s, -1)\n",
    "        x3 = self.fc(x2)\n",
    "        out = x3.view(b,s,-1)\n",
    "        if verbose:\n",
    "            print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class silly(nn.Module):\n",
    "    def __init__(self,num_emb,emb_dim,max_len,mask_id):\n",
    "        super(silly,self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=num_emb, embedding_dim=emb_dim)#, padding_idx=mask_id)\n",
    "        self.lin1 = nn.Linear(in_features=emb_dim, out_features=emb_dim)\n",
    "        self.lu = nn.ReLU()\n",
    "        self.fc = nn.Linear(in_features=emb_dim, out_features=num_emb)\n",
    "    \n",
    "    def forward(self,x_in,verbose=False):\n",
    "        x = self.emb(x_in)\n",
    "        b,s,d = x.size()\n",
    "        x = x.contiguous().view(b*s, -1)\n",
    "        x = self.lin1(x)\n",
    "        x = self.lu(x)\n",
    "        x = self.fc(x)\n",
    "        out = x.view(b,s,-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(vocab,vectors):\n",
    "    b,s,d = vectors.size()\n",
    "    assert d == len(vocab)\n",
    "    x = vectors[0]\n",
    "    probs = F.softmax(x, dim=1)\n",
    "    sent = ''\n",
    "    for i in range(s):\n",
    "        v = probs[i,:]\n",
    "        # replace with argmax?\n",
    "        win = torch.multinomial(v, num_samples=1)\n",
    "        idx = win.item()\n",
    "        sent += vocab.lookup_idx(idx)\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mini_transformer(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.737310528755188 epoch 1\n",
      "1.6994060277938843 epoch 1\n",
      "1.680991530418396 epoch 1\n",
      "1.63821280002594 epoch 1\n",
      "1.6306419372558594 epoch 2\n",
      "1.6117055416107178 epoch 2\n",
      "1.5949723720550537 epoch 3\n",
      "1.5064290761947632 epoch 3\n",
      "1.5056815147399902 epoch 3\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "num_epochs = 4\n",
    "device = 'cpu'\n",
    "from tqdm import tqdm\n",
    "bestloss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    ### train ----\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for data in dl:\n",
    "        x,y = data\n",
    "        x.to(device)\n",
    "        y.to(device)\n",
    "        y_pred = model(x)\n",
    "        b,s,d = y_pred.shape\n",
    "        y_pred_to_loss = y_pred.view(b*s,d)\n",
    "        y_to_loss = y.view(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(y_pred_to_loss, y_to_loss)#, ignore_index=mask_id)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss.item() < bestloss:\n",
    "            bestloss = loss.item()\n",
    "            print(loss.item(), f\"epoch {epoch+1}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_idx(ind):\n",
    "    s = ''\n",
    "    for idx in ind:\n",
    "        if idx == 0:\n",
    "            break\n",
    "        s += vocab.lookup_idx(idx)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_samp(model,vocab,sample_size=30,prompt=\"\"):\n",
    "    ind = [vocab.begin_idx]\n",
    "    ind.extend([vocab.lookup_token(char) for char in prompt])\n",
    "    ind.extend([vocab.mask_idx for _ in range(len(prompt), model.max_len - 1)]) # plus or minus 1...\n",
    "    assert model.max_len == len(ind)\n",
    "\n",
    "    for i in range(len(prompt), sample_size):\n",
    "        x = torch.tensor(ind).unsqueeze(dim=0)\n",
    "        pred = model(x)\n",
    "        \n",
    "        b,s,d = pred.size()\n",
    "        assert d == len(vocab)\n",
    "        z = pred[0,i,:] # plus or minus one?\n",
    "        prob = F.softmax(z,dim=0)\n",
    "        win = torch.multinomial(prob, num_samples=1)\n",
    "        ind[i+1] = win.item()\n",
    "        \n",
    "    return decode_idx(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<begin>What terir phhatoafoer thof wh\n",
      "<begin>What beiktcet t y we ynd s won\n",
      "<begin>Whateealidincnlhitr,,Hc is mur\n",
      "<begin>What t ibautecabeoule cerloulo\n",
      "<begin>What1e id be ps oudfisouteng t\n",
      "<begin>Whatrofretourrtiubu m dy pf  \"\n",
      "<begin>Whatt I s t Mswoidansus o I iv\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    print(gen_samp(model,vocab,prompt='What'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
