{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for google colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/johncalab/Dropbox/gitstuff/deepShowerThoughts/ai'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# # for google colab\n",
    "# os.chdir('gdrive/My Drive/AI')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from charvocabulary import charVocabulary\n",
    "from charvectorizer import charVectorizer\n",
    "from chardataset import charDataset\n",
    "from charmodel import charModel\n",
    "from charsample import gen_samp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'may15nov17_above130_less100_light.csv'\n",
    "csv_path = os.path.join('training_data',DATASET)\n",
    "\n",
    "rootpath = 'oren'\n",
    "if rootpath not in os.listdir():\n",
    "    os.mkdir(rootpath)\n",
    "\n",
    "dict_path = os.path.join(rootpath, 'dict.pkl')\n",
    "model_path = os.path.join(rootpath, 'model.pt')\n",
    "losses_path = os.path.join(rootpath, 'losses.txt')\n",
    "bestloss_path = os.path.join(rootpath, 'bestloss.txt')\n",
    "params_path = os.path.join(rootpath, 'params.pkl')\n",
    "\n",
    "RESUME = True\n",
    "NUM_EPOCHS = 2\n",
    "CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv(csv_path).title.astype('U')\n",
    "\n",
    "vocab = charVocabulary()\n",
    "if RESUME:\n",
    "    token_to_idx = pickle.load(open(dict_path,'rb'))\n",
    "    vocab = charVocabulary(token_to_idx=token_to_idx)\n",
    "else:\n",
    "    vocab.add_series(df=posts)\n",
    "    pickle.dump(vocab.token_to_idx, open(dict_path,'wb'))\n",
    "\n",
    "if RESUME:\n",
    "    params = pickle.load(open(params_path,'rb'))\n",
    "else:\n",
    "    params = {}\n",
    "    params['vocab_size'] = len(vocab)\n",
    "    params['embedding_dim'] = 128\n",
    "    params['rnn_hidden_dim'] = 512\n",
    "    params['num_layers'] = 2\n",
    "    params['dropout_p'] = 0.5\n",
    "    params['bidirectional'] = False\n",
    "    # missing the vocab_size!\n",
    "    \n",
    "    pickle.dump(params, open(params_path,'wb'))\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logpath = os.path.join(rootpath, 'logbook.log')\n",
    "logging.basicConfig(level=logging.INFO, filename=logpath, filemode='w')\n",
    "\n",
    "logging.info(f\"Vocab size is {params['vocab_size']}.\")\n",
    "logging.info(f\"Embedding dim is {params['embedding_dim']}.\")\n",
    "logging.info(f\"RNN hidden dim is {params['rnn_hidden_dim']}.\")\n",
    "logging.info(f\"I am using {params['num_layers']} RNN layers.\")\n",
    "if params['bidirectional']:\n",
    "    logging.info(f\"RNN unit is bidirectional.\")\n",
    "logging.info(f\"Dropout is {params['dropout_p']}.\")\n",
    "logging.info(f\"Batch size is {BATCH_SIZE}.\")\n",
    "\n",
    "logging.info(f\"Folder is {rootpath}.\")\n",
    "logging.info(f\"Training on {csv_path}.\")\n",
    "logging.info(f\"Dictionary is in {dict_path}.\")\n",
    "logging.info(f\"Model is in {model_path}.\")\n",
    "logging.info(f\"Loss info is in {losses_path}.\")\n",
    "logging.info(f\"Best loss is in {bestloss_path}.\")\n",
    "logging.info(f\"I aspire to train for {NUM_EPOCHS} epochs.\")\n",
    "if RESUME:\n",
    "    logging.info(f\"I am resuming training.\")\n",
    "else:\n",
    "    logging.info(f\"I am training from scratch.\")\n",
    "if CUDA:\n",
    "    logging.info(f\"I will try to use CUDA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am using cpu.\n"
     ]
    }
   ],
   "source": [
    "if CUDA and torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "t_device = torch.device(device)\n",
    "\n",
    "s = f\"I am using {device}.\"\n",
    "logging.info(s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "charModel(\n",
       "  (emb): Embedding(136, 128, padding_idx=0)\n",
       "  (rnn): GRU(128, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (fc): Linear(in_features=512, out_features=136, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskid = vocab.mask_idx\n",
    "vectorizer = charVectorizer(vocab=vocab)\n",
    "\n",
    "model = charModel(**params)\n",
    "\n",
    "if RESUME:\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = charDataset(vectorizer=vectorizer, posts=posts)\n",
    "dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)#, momentum=0.1)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–‹         | 21/334 [00:20<04:57,  1.05it/s]"
     ]
    }
   ],
   "source": [
    "# OK let's start training ----------------------------------------------------------\n",
    "try:\n",
    "    if RESUME:\n",
    "        with open(bestloss_path, 'r') as f:\n",
    "            bestloss = float(f.readline())\n",
    "    else:\n",
    "        bestloss = float('inf')\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        ### training ----------\n",
    "        model.train()\n",
    "\n",
    "        batch_losses = []\n",
    "\n",
    "        with tqdm.tqdm(total=len(dl)) as progress_bar:\n",
    "            for x,y in dl:\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                y_pred = model(x)\n",
    "\n",
    "                batch_size, seq_len, feats = y_pred.shape\n",
    "                y_pred_loss = y_pred.view(batch_size*seq_len,feats)\n",
    "                y_loss = y.view(-1)\n",
    "\n",
    "                loss = F.cross_entropy(y_pred_loss, y_loss, ignore_index=maskid)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_losses.append(loss.item())\n",
    "                with open(losses_path, 'a') as f:\n",
    "                    stringa = '\\n' + str(loss.item())\n",
    "                    f.write(stringa)\n",
    "\n",
    "                progress_bar.update(1)\n",
    "        \n",
    "        avgloss = np.asarray(batch_losses).mean()\n",
    "\n",
    "        # if we'll want to use a train/test split\n",
    "        # model should be updated with VALIDATION losses, not training\n",
    "\n",
    "        if avgloss < bestloss:\n",
    "            bestloss = avgloss\n",
    "            logging.info(\"Loss improved! I am saving this model.\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "#             logging.info(f\"{bestloss}\")\n",
    "            with open(bestloss_path, 'w') as f:\n",
    "                f.write(str(bestloss))\n",
    "        \n",
    "        ### 'validating' -------------------\n",
    "        model.eval()\n",
    "        model.to('cpu')\n",
    "        print('\\n')\n",
    "        for i in range(5):\n",
    "            print(gen_samp(model=model,vocab=vocab,prompt=\"\"))\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        print(f\"\\nEpoch number {epoch+1} has concluded. The mean average loss was {avgloss}.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.info(f\"Model trained for {epoch} full epochs.\")\n",
    "    logging.error(\"Something went wrong\", exc_info=True)\n",
    "\n",
    "#logger.info(\"I'll save the latest model.\")\n",
    "\n",
    "#torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
