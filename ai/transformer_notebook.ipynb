{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook implementation of transformer (works better with google colab...)\n",
    "http://www.peterbloem.nl/blog/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/johncalab/Dropbox/gitstuff/deepShowerThoughts/ai'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change directory\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class charVocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token \n",
    "                                for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "        self.mask_token = '<mask>'\n",
    "        self.begin_token = '<begin>'\n",
    "        self.end_token = '<end>'\n",
    "        self.unk_token = '<unk>'\n",
    "        self.space_token = ' '\n",
    "\n",
    "        self.mask_idx = self.add_token(self.mask_token)\n",
    "        self.begin_idx = self.add_token(self.begin_token)\n",
    "        self.end_idx = self.add_token(self.end_token)\n",
    "        self.unk_idx = self.add_token(self.unk_token)\n",
    "        self.space_idx = self.add_token(self.space_token)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.token_to_idx) == len(self.idx_to_token)\n",
    "        return len(self.token_to_idx)\n",
    "\n",
    "    def lookup_token(self,token):\n",
    "        return self.token_to_idx[token]\n",
    "\n",
    "    def lookup_idx(self,i):\n",
    "        return self.idx_to_token[i]\n",
    "\n",
    "    def add_txt(self,path):\n",
    "        with open(path, 'r') as f:\n",
    "            fulltext = f.read()\n",
    "            for c in fulltext:\n",
    "                if c != '\\n':\n",
    "                    self.add_token(c)\n",
    "        return None\n",
    "\n",
    "    def add_series(self,df):\n",
    "        for sentence in df:\n",
    "            max_len = min(300, len(sentence))\n",
    "            for char in sentence[:max_len]:\n",
    "                self.add_token(char)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class charVectorizer(object):\n",
    "    def __init__(self,vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def vectorize(self, sent, max_len=-1):\n",
    "        \"\"\"\n",
    "        max_len is used to know how much to pad\n",
    "        \"\"\"\n",
    "        ind = [self.vocab.begin_idx]\n",
    "        ind.extend(self.vocab.lookup_token(token) for token in sent)\n",
    "        ind.append(self.vocab.end_idx)\n",
    "        \n",
    "        max_len = max(len(ind), max_len)\n",
    "\n",
    "        x = np.empty(max_len-1, dtype=np.int64)\n",
    "        x[:len(ind)-1] = ind[:-1]\n",
    "        x[len(ind)-1:] = self.vocab.mask_idx\n",
    "\n",
    "        y = np.empty(max_len-1, dtype=np.int64)\n",
    "        y[:len(ind)-1] = ind[1:]\n",
    "        y[len(ind)-1:] = self.vocab.mask_idx\n",
    "\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class charDataset(Dataset):\n",
    "    def __init__(self,vectorizer,posts):\n",
    "        self.posts = posts\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "        max_len = len(posts.iloc[0])\n",
    "        for sentence in posts:\n",
    "            max_len = max(max_len, len(sentence))\n",
    "\n",
    "        self.max_len = max_len + 3\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        sent = self.posts.iloc[i]\n",
    "        x,y = self.vectorizer.vectorize(sent=sent, max_len=self.max_len)\n",
    "        assert x.shape == y.shape\n",
    "        assert x.shape[0] == self.max_len-1\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def mask_(matrices, maskval=0.0, mask_diagonal=False):\n",
    "    b, h, w = matrices.size()\n",
    "\n",
    "    indices = torch.triu_indices(h, w, offset=0 if mask_diagonal else 1)\n",
    "    matrices[:, indices[0], indices[1]] = maskval\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, k, heads=8, mask=True):\n",
    "        super().__init__()\n",
    "        self.k, self.heads, self.mask = k, heads, mask\n",
    "        # instead of having one matrix per head,\n",
    "        # we stack them into one big matrix\n",
    "        self.tokeys = nn.Linear(k, k*heads, bias=False)\n",
    "        self.toqueries = nn.Linear(k, k*heads, bias=False)\n",
    "        self.tovalues = nn.Linear(k, k*heads, bias=False)\n",
    "        self.unifyheads = nn.Linear(heads*k, k)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b,t,k = x.size()\n",
    "        h = self.heads\n",
    "        assert k == self.k\n",
    "        \n",
    "        queries = self.toqueries(x).view(b,t,h,k)\n",
    "        keys = self.tokeys(x).view(b,t,h,k)\n",
    "        values = self.tovalues(x).view(b,t,h,k)\n",
    "        \n",
    "        queries = queries.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "        keys = keys.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "        values = values.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "        \n",
    "        queries = queries / (k ** (1/4))\n",
    "        keys = keys / (k ** (1/4))\n",
    "        # weights\n",
    "        dot = torch.bmm(queries, keys.transpose(1,2))\n",
    "        assert dot.size() == (b*h, t, t)\n",
    "\n",
    "        if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n",
    "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
    "#             maskval=float('-inf')\n",
    "#             b, h, w = dot.size()\n",
    "#             indices = torch.triu_indices(h, w, offset=1)\n",
    "#             dot[:, indices[0], indices[1]] = maskval\n",
    "#             print(f'dot has shape {dot.shape}')\n",
    "\n",
    "        dot = F.softmax(dot, dim=2)\n",
    "        \n",
    "        # apply self-attienton to values\n",
    "        out = torch.bmm(dot, values)\n",
    "        out = out.view(b,h,t,k)\n",
    "        out = out.transpose(1,2).contiguous().view(b,t,h*k)\n",
    "        out = self.unifyheads(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.peterbloem.nl/files/transformers/transformer-block.svg\n",
    "# https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm\n",
    "# ff stands for 'feed forward'\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, k, heads, seq_length, mask=True, ff_multiple=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = SelfAttention(k,heads, mask=mask)\n",
    "#         self.mask = mask\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(k)\n",
    "        self.norm2 = nn.LayerNorm(k)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "                               nn.Linear(k,ff_multiple*k),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(ff_multiple*k,k))\n",
    "        \n",
    "        self.dout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # apply attention\n",
    "        attended = self.attention(x)\n",
    "        # normalize 1 + residual connection\n",
    "        normalized = self.norm1(attended + x)\n",
    "        # apply dropout\n",
    "        dropped = self.dout(normalized)\n",
    "        # look up MLP and feedforward\n",
    "        forwarded = self.ff(dropped)\n",
    "        # normalize 2 + residual connection\n",
    "        normalized_again = self.norm2(forwarded + dropped)\n",
    "        out = self.dout(normalized_again)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embedding_dim, heads, depth, seq_length, num_tokens, mask_id):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_tokens = num_tokens\n",
    "        self.token_emb = nn.Embedding(embedding_dim=embedding_dim, num_embeddings=num_tokens, padding_idx=mask_id)\n",
    "        self.pos_emb = nn.Embedding(embedding_dim=embedding_dim, num_embeddings=seq_length)\n",
    "        \n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(TransformerBlock(k=embedding_dim,heads=heads, seq_length=seq_length, mask=True))\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "        \n",
    "        # num_classes = num_tokens, when you're generating text\n",
    "        self.to_probs = nn.Linear(embedding_dim, num_tokens)\n",
    "    \n",
    "    def forward(self,x, apply_softmax=True):\n",
    "        tokens = self.token_emb(x)\n",
    "        b,t,k = tokens.size()\n",
    "        \n",
    "        # generate the position embedding\n",
    "        # tweak this to make device work\n",
    "#         positions = torch.arange(t, device=device)\n",
    "        positions = torch.arange(t)\n",
    "        positions = self.pos_emb(positions)\n",
    "        positions = positions[None, :, :]\n",
    "        positions = positions.expand(b, t, k)\n",
    "        \n",
    "        x = tokens + positions\n",
    "        x = self.tblocks(x)\n",
    "        \n",
    "        out = self.to_probs(x.view(b*t, k)).view(b, t, self.num_tokens)\n",
    "        \n",
    "        # not sure I want to apply log-softmax\n",
    "        # just use cross-entropy loss directly, no?\n",
    "        if apply_softmax:\n",
    "            out = F.log_softmax(out, dim=2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader, random_split\n",
    "import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'may15nov17_above130_less100_light.csv'\n",
    "csv_path = os.path.join('training_data',DATASET)\n",
    "\n",
    "rootpath = 'bertrand'\n",
    "if rootpath not in os.listdir():\n",
    "    os.mkdir(rootpath)\n",
    "\n",
    "dict_path = os.path.join(rootpath, 'dict.pkl')\n",
    "model_path = os.path.join(rootpath, 'model.pt')\n",
    "train_losses_path = os.path.join(rootpath, 'train_losses.txt')\n",
    "test_losses_path = os.path.join(rootpath, 'test_losses.txt')\n",
    "bestloss_path = os.path.join(rootpath, 'best_loss.txt')\n",
    "params_path = os.path.join(rootpath, 'params.pkl')\n",
    "\n",
    "RESUME = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "posts = pd.read_csv(csv_path).title.astype('U')\n",
    "\n",
    "if RESUME:\n",
    "    token_to_idx = pickle.load(open(dict_path,'rb'))\n",
    "    vocab = charVocabulary(token_to_idx=token_to_idx)\n",
    "else:\n",
    "    vocab = charVocabulary()\n",
    "    vocab.add_series(df=posts)\n",
    "    pickle.dump(vocab.token_to_idx, open(dict_path,'wb'))\n",
    "\n",
    "mask_id = vocab.mask_idx\n",
    "vectorizer = charVectorizer(vocab=vocab)\n",
    "\n",
    "full_ds = charDataset(vectorizer=vectorizer, posts=posts)\n",
    "\n",
    "if RESUME:\n",
    "    params = pickle.load(open(params_path,'rb'))\n",
    "else:\n",
    "    params = {}\n",
    "    params['num_tokens'] = len(vocab)\n",
    "    params['embedding_dim'] = 128\n",
    "    params['seq_length'] = ds.max_len\n",
    "    params['heads'] = 8\n",
    "    params['depth'] = 4\n",
    "    params['mask_id'] = mask_id\n",
    "#     params['dropout_p'] = 0.5\n",
    "    \n",
    "    pickle.dump(params, open(params_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = True\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "SPLIT_FRAC = 0.20\n",
    "test_size = int(SPLIT_FRAC * len(full_ds))\n",
    "train_size = len(full_ds) - test_size\n",
    "train_ds, test_ds = random_split(full_ds, [train_size, test_size])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logpath = os.path.join(rootpath, 'logbook.log')\n",
    "logger = logging.getLogger()\n",
    "hdlr = logging.FileHandler(logpath)\n",
    "logger.addHandler(hdlr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am using cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if CUDA and torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "t_device = torch.device(device)\n",
    "\n",
    "s = f\"I am using {device}.\"\n",
    "logging.info(s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (token_emb): Embedding(136, 128, padding_idx=0)\n",
       "  (pos_emb): Embedding(103, 128)\n",
       "  (tblocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (dout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (dout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (dout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (dout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (to_probs): Linear(in_features=128, out_features=136, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(**params)\n",
    "\n",
    "if RESUME:\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/267 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 17/267 [00:27<07:02,  1.69s/it]"
     ]
    }
   ],
   "source": [
    "# OK let's start training ----------------------------------------------------------\n",
    "try:\n",
    "    if RESUME:\n",
    "        with open(bestloss_path, 'r') as f:\n",
    "            bestloss = float(f.readline())\n",
    "    else:\n",
    "        bestloss = float('inf')\n",
    "        \n",
    "    train_epoch_losses = []\n",
    "    test_epoch_losses = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        ### training ----------\n",
    "        print('\\nTraining\\n')\n",
    "        model.train()\n",
    "        train_batch_losses = []\n",
    "        with tqdm.tqdm(total=len(train_dl)) as progress_bar:\n",
    "            for x,y in train_dl:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                y_pred = model(x)\n",
    "\n",
    "                batch_size, seq_len, feats = y_pred.shape\n",
    "                y_pred_loss = y_pred.view(batch_size*seq_len,feats)\n",
    "                y_loss = y.view(-1)\n",
    "\n",
    "                loss = F.cross_entropy(y_pred_loss, y_loss, ignore_index=mask_id)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_batch_losses.append(loss.item())\n",
    "\n",
    "                progress_bar.update(1)\n",
    "        \n",
    "        avgloss = np.asarray(train_batch_losses).mean()\n",
    "        print(f\"\\nEpoch number {epoch+1} is done training. The mean average loss was {avgloss}.\\n\")\n",
    "        with open(train_losses_path, 'a') as f:\n",
    "            stringa = '\\n' + str(avgloss)\n",
    "            f.write(stringa)\n",
    "\n",
    "        ### testing ----------\n",
    "        print('\\nTesting\\n')\n",
    "        model.eval()\n",
    "        test_batch_losses = []\n",
    "        with tqdm.tqdm(total=len(test_dl)) as progress_bar:\n",
    "            for x,y in test_dl:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                y_pred = model(x)\n",
    "\n",
    "                batch_size, seq_len, feats = y_pred.shape\n",
    "                y_pred_loss = y_pred.view(batch_size*seq_len,feats)\n",
    "                y_loss = y.view(-1)\n",
    "\n",
    "                loss = F.cross_entropy(y_pred_loss, y_loss, ignore_index=mask_id)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                test_batch_losses.append(loss.item())\n",
    "\n",
    "                progress_bar.update(1)\n",
    "        \n",
    "        avgloss = np.asarray(test_batch_losses).mean()\n",
    "        print(f\"\\nEpoch number {epoch+1} is done testing. The mean average loss was {avgloss}.\\n\")\n",
    "        with open(test_losses_path, 'a') as f:\n",
    "            stringa = '\\n' + str(avgloss)\n",
    "            f.write(stringa)\n",
    "        \n",
    "        if avgloss < bestloss:\n",
    "            bestloss = avgloss\n",
    "            logging.info(\"Loss improved! I am saving this model.\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            with open(bestloss_path, 'w') as f:\n",
    "                f.write(str(bestloss))\n",
    "\n",
    "        print(f\"\\nEpoch number {epoch+1} is done testing. The mean average loss was {avgloss}.\\n\")\n",
    "        model.to(device)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.error('something went wrong', exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
