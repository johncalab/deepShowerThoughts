{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iNI9CWMulGpZ"
   },
   "source": [
    "http://www.peterbloem.nl/blog/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20120,
     "status": "ok",
     "timestamp": 1569904292589,
     "user": {
      "displayName": "John Calabrese",
      "photoUrl": "",
      "userId": "16181148882419268806"
     },
     "user_tz": 240
    },
    "id": "S1Yz_-x3lGpa",
    "outputId": "9b1c7627-96bd-4756-983b-d925dbefa2d9"
   },
   "outputs": [],
   "source": [
    "# for google colab\n",
    "# mount\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1418,
     "status": "ok",
     "timestamp": 1569904295639,
     "user": {
      "displayName": "John Calabrese",
      "photoUrl": "",
      "userId": "16181148882419268806"
     },
     "user_tz": 240
    },
    "id": "OuWGFQ3HlGpc",
    "outputId": "96910370-4ee0-49ac-96e8-8890a1fa3b58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content/gdrive/My Drive/ai'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change directory\n",
    "import os\n",
    "# # for google colab\n",
    "os.chdir('gdrive/My Drive/ai')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mcKTbBU6lGpf"
   },
   "outputs": [],
   "source": [
    "class charVocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token \n",
    "                                for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "        self.mask_token = '<mask>'\n",
    "        self.begin_token = '<begin>'\n",
    "        self.end_token = '<end>'\n",
    "        self.unk_token = '<unk>'\n",
    "        self.space_token = ' '\n",
    "\n",
    "        self.mask_idx = self.add_token(self.mask_token)\n",
    "        self.begin_idx = self.add_token(self.begin_token)\n",
    "        self.end_idx = self.add_token(self.end_token)\n",
    "        self.unk_idx = self.add_token(self.unk_token)\n",
    "        self.space_idx = self.add_token(self.space_token)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.token_to_idx) == len(self.idx_to_token)\n",
    "        return len(self.token_to_idx)\n",
    "\n",
    "    def lookup_token(self,token):\n",
    "        return self.token_to_idx[token]\n",
    "\n",
    "    def lookup_idx(self,i):\n",
    "        return self.idx_to_token[i]\n",
    "\n",
    "    def add_txt(self,path):\n",
    "        with open(path, 'r') as f:\n",
    "            fulltext = f.read()\n",
    "            for c in fulltext:\n",
    "                if c != '\\n':\n",
    "                    self.add_token(c)\n",
    "        return None\n",
    "\n",
    "    def add_series(self,df):\n",
    "        for sentence in df:\n",
    "            max_len = min(300, len(sentence))\n",
    "            for char in sentence[:max_len]:\n",
    "                self.add_token(char)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwryIOYxlGpg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class charVectorizer(object):\n",
    "    def __init__(self,vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def vectorize(self, sent, max_len=-1):\n",
    "        \"\"\"\n",
    "        max_len is used to know how much to pad\n",
    "        \"\"\"\n",
    "        ind = [self.vocab.begin_idx]\n",
    "        ind.extend(self.vocab.lookup_token(token) for token in sent)\n",
    "        ind.append(self.vocab.end_idx)\n",
    "        \n",
    "        max_len = max(len(ind), max_len)\n",
    "\n",
    "        x = np.empty(max_len-1, dtype=np.int64)\n",
    "        x[:len(ind)-1] = ind[:-1]\n",
    "        x[len(ind)-1:] = self.vocab.mask_idx\n",
    "\n",
    "        y = np.empty(max_len-1, dtype=np.int64)\n",
    "        y[:len(ind)-1] = ind[1:]\n",
    "        y[len(ind)-1:] = self.vocab.mask_idx\n",
    "\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LOKxCkfhlGpi"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class charDataset(Dataset):\n",
    "    def __init__(self,vectorizer,posts,extra_pad=100):\n",
    "        self.posts = posts\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "        posts_len = len(posts.iloc[0])\n",
    "        for sentence in posts:\n",
    "            posts_len = max(posts_len, len(sentence))\n",
    "        self.max_len = posts_len + extra_pad\n",
    "        self.seq_len = self.max_len - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        sent = self.posts.iloc[i]\n",
    "        x,y = self.vectorizer.vectorize(sent=sent, max_len=self.max_len)\n",
    "        assert x.shape == y.shape\n",
    "        assert x.shape[0] == self.max_len-1\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4PTtFe1lGpj"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def mask_(matrices, maskval=0.0, mask_diagonal=False):\n",
    "    b, h, w = matrices.size()\n",
    "\n",
    "    indices = torch.triu_indices(h, w, offset=0 if mask_diagonal else 1)\n",
    "    matrices[:, indices[0], indices[1]] = maskval\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, k, heads=8, mask=True):\n",
    "        super().__init__()\n",
    "        self.k, self.heads, self.mask = k, heads, mask\n",
    "        # instead of having one matrix per head,\n",
    "        # we stack them into one big matrix\n",
    "        self.tokeys = nn.Linear(k, k*heads, bias=False)\n",
    "        self.toqueries = nn.Linear(k, k*heads, bias=False)\n",
    "        self.tovalues = nn.Linear(k, k*heads, bias=False)\n",
    "        self.unifyheads = nn.Linear(heads*k, k)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b,t,k = x.size()\n",
    "        h = self.heads\n",
    "        assert k == self.k\n",
    "        \n",
    "        queries = self.toqueries(x).view(b,t,h,k)\n",
    "        keys = self.tokeys(x).view(b,t,h,k)\n",
    "        values = self.tovalues(x).view(b,t,h,k)\n",
    "        \n",
    "        queries = queries.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "        keys = keys.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "        values = values.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "        \n",
    "        queries = queries / (k ** (1/4))\n",
    "        keys = keys / (k ** (1/4))\n",
    "        # weights\n",
    "        dot = torch.bmm(queries, keys.transpose(1,2))\n",
    "        assert dot.size() == (b*h, t, t)\n",
    "\n",
    "        if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n",
    "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
    "\n",
    "        dot = F.softmax(dot, dim=2)\n",
    "        \n",
    "        # apply self-attienton to values\n",
    "        out = torch.bmm(dot, values)\n",
    "        out = out.view(b,h,t,k)\n",
    "        out = out.transpose(1,2).contiguous().view(b,t,h*k)\n",
    "        out = self.unifyheads(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WxSx7VBllGpl"
   },
   "outputs": [],
   "source": [
    "# http://www.peterbloem.nl/files/transformers/transformer-block.svg\n",
    "# https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm\n",
    "# ff stands for 'feed forward'\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, k, heads, seq_length, mask=True, ff_multiple=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = SelfAttention(k,heads, mask=mask)\n",
    "#         self.mask = mask\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(k)\n",
    "        self.norm2 = nn.LayerNorm(k)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "                               nn.Linear(k,ff_multiple*k),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(ff_multiple*k,k))\n",
    "        \n",
    "        self.dout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # apply attention\n",
    "        attended = self.attention(x)\n",
    "        # normalize 1 + residual connection\n",
    "        normalized = self.norm1(attended + x)\n",
    "        # apply dropout\n",
    "        dropped = self.dout(normalized)\n",
    "        # look up MLP and feedforward\n",
    "        forwarded = self.ff(dropped)\n",
    "        # normalize 2 + residual connection\n",
    "        normalized_again = self.norm2(forwarded + dropped)\n",
    "        out = self.dout(normalized_again)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FXnIhX0wlGpm"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim,\n",
    "                 heads, depth,\n",
    "                 seq_length,\n",
    "                 num_tokens,\n",
    "                 mask_id,\n",
    "                 ff_multiple=4,\n",
    "                 dropout=0.0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.mask_id = mask_id\n",
    "        self.seq_len = seq_length\n",
    "        self.token_emb = nn.Embedding(embedding_dim=embedding_dim, num_embeddings=num_tokens, padding_idx=mask_id)\n",
    "        self.pos_emb = nn.Embedding(embedding_dim=embedding_dim, num_embeddings=self.seq_len)\n",
    "        \n",
    "        # in tblocks I should also specify:\n",
    "        # ff_multiple and dropout\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(TransformerBlock(k=embedding_dim,heads=heads, seq_length=self.seq_len, mask=True, ff_multiple=4, dropout=dropout))\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "        \n",
    "        # num_classes = num_tokens, when you're generating text\n",
    "        self.to_probs = nn.Linear(embedding_dim, num_tokens)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        tokens = self.token_emb(x)\n",
    "        b,t,k = tokens.size()\n",
    "\n",
    "        assert t == self.seq_len\n",
    "        \n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            positions = torch.arange(t, device='cuda')\n",
    "        else:\n",
    "            positions.to('cuda')\n",
    "        positions = self.pos_emb(positions)\n",
    "        positions = positions[None, :, :]\n",
    "        positions = positions.expand(b, t, k)\n",
    "        \n",
    "        x = tokens + positions\n",
    "        x = self.tblocks(x)\n",
    "        \n",
    "        out = self.to_probs(x.view(b*t, k)).view(b, t, self.num_tokens)\n",
    "        return out\n",
    "        \n",
    "#         # not sure I want to apply log-softmax\n",
    "#         # just use cross-entropy loss directly, no?\n",
    "#         if apply_softmax:\n",
    "#             out = F.log_softmax(out, dim=2)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h932GBTplGpo"
   },
   "outputs": [],
   "source": [
    "def gen_samp(model,\n",
    "            vocab,\n",
    "            prompt=''):\n",
    "    \n",
    "    seq_len = model.seq_len\n",
    "    if len(prompt) >= seq_len:\n",
    "        return prompt\n",
    "\n",
    "    mask_id = model.mask_id\n",
    "    one_hot = [mask_id for _ in range(seq_len)]\n",
    "    \n",
    "    bos = vocab.begin_idx\n",
    "    one_hot[0] = bos\n",
    "    for i,c in enumerate(prompt):\n",
    "        idx = vocab.lookup_token(c)\n",
    "        one_hot[i+1] = idx\n",
    "\n",
    "    for i in range(len(prompt), seq_len):\n",
    "        if torch.cuda.is_available():\n",
    "            hot_tensor = torch.tensor(one_hot, dtype=torch.int64, device='cuda').unsqueeze(dim=0)\n",
    "        else:\n",
    "            hot_tensor = torch.tensor(one_hot, dtype=torch.int64).unsqueeze(dim=0)\n",
    "        pred = model(hot_tensor)\n",
    "        last = pred.squeeze(dim=0)[i,:]\n",
    "        probs = F.softmax(last, dim=0)\n",
    "        winner = torch.multinomial(probs, num_samples=1)\n",
    "        one_hot[i] = winner.item()\n",
    "    \n",
    "    output = \"\"\n",
    "    for idx in one_hot:\n",
    "        token = vocab.lookup_idx(idx)\n",
    "        output += token\n",
    "    \n",
    "    start = vocab.begin_token\n",
    "    end = vocab.end_token\n",
    "    return output[output.find(start)+len(start):output.find(end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-6eGk7our78A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxhW5R-7lGpr"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "agR9epHdlGpu"
   },
   "outputs": [],
   "source": [
    "DATASET = 'may15nov17_above130_less100_light.csv'\n",
    "csv_path = os.path.join('training_data',DATASET)\n",
    "\n",
    "rootpath = 'bertrand'\n",
    "if rootpath not in os.listdir():\n",
    "    os.mkdir(rootpath)\n",
    "\n",
    "dict_path = os.path.join(rootpath, 'dict.pkl')\n",
    "model_path = os.path.join(rootpath, 'model.pt')\n",
    "train_losses_path = os.path.join(rootpath, 'train_losses.txt')\n",
    "test_losses_path = os.path.join(rootpath, 'test_losses.txt')\n",
    "bestloss_path = os.path.join(rootpath, 'best_loss.txt')\n",
    "params_path = os.path.join(rootpath, 'params.pkl')\n",
    "\n",
    "RESUME = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kE4JOWiklGpv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "posts = pd.read_csv(csv_path).title.astype('U')\n",
    "\n",
    "if RESUME:\n",
    "    token_to_idx = pickle.load(open(dict_path,'rb'))\n",
    "    vocab = charVocabulary(token_to_idx=token_to_idx)\n",
    "else:\n",
    "    vocab = charVocabulary()\n",
    "    vocab.add_series(df=posts)\n",
    "    pickle.dump(vocab.token_to_idx, open(dict_path,'wb'))\n",
    "\n",
    "mask_id = vocab.mask_idx\n",
    "vectorizer = charVectorizer(vocab=vocab)\n",
    "\n",
    "full_ds = charDataset(vectorizer=vectorizer, posts=posts)\n",
    "\n",
    "if RESUME:\n",
    "    params = pickle.load(open(params_path,'rb'))\n",
    "else:\n",
    "    params = {}\n",
    "    params['num_tokens'] = len(vocab)\n",
    "    params['embedding_dim'] = 128\n",
    "    params['seq_length'] = full_ds.seq_len\n",
    "    params['heads'] = 8\n",
    "    params['depth'] = 4\n",
    "    params['ff_multiple'] = 4\n",
    "    params['dropout'] = 0.5\n",
    "    params['mask_id'] = mask_id\n",
    "\n",
    "    pickle.dump(params, open(params_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_Al3k5-lGpx"
   },
   "outputs": [],
   "source": [
    "CUDA = True\n",
    "NUM_EPOCHS = 88\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "SPLIT_FRAC = 0.20\n",
    "test_size = int(SPLIT_FRAC * len(full_ds))\n",
    "train_size = len(full_ds) - test_size\n",
    "train_ds, test_ds = random_split(full_ds, [train_size, test_size])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oe00YlKtlGpy"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logpath = os.path.join(rootpath, 'logbook.log')\n",
    "logger = logging.getLogger()\n",
    "hdlr = logging.FileHandler(logpath)\n",
    "logger.addHandler(hdlr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 595,
     "status": "ok",
     "timestamp": 1569909577167,
     "user": {
      "displayName": "John Calabrese",
      "photoUrl": "",
      "userId": "16181148882419268806"
     },
     "user_tz": 240
    },
    "id": "ETqtZn1plGpz",
    "outputId": "0da1bcc1-1f0c-443b-cbc4-2b91878d0064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am using cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if CUDA and torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "t_device = torch.device(device)\n",
    "\n",
    "s = f\"I am using {device}.\"\n",
    "logging.info(s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1349,
     "status": "ok",
     "timestamp": 1569909578310,
     "user": {
      "displayName": "John Calabrese",
      "photoUrl": "",
      "userId": "16181148882419268806"
     },
     "user_tz": 240
    },
    "id": "1QUfMB5OlGp1",
    "outputId": "1870a2a9-1070-4f33-dc9d-d276810dca8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (token_emb): Embedding(136, 128, padding_idx=0)\n",
       "  (pos_emb): Embedding(199, 128)\n",
       "  (tblocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (dout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (dout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (dout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (dout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (to_probs): Linear(in_features=128, out_features=136, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 194,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(**params)\n",
    "\n",
    "if RESUME:\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NE2B0p8OlGp2"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ugSTbv5plGp4"
   },
   "outputs": [],
   "source": [
    "# OK let's start training ----------------------------------------------------------\n",
    "try:\n",
    "    if RESUME:\n",
    "        with open(bestloss_path, 'r') as f:\n",
    "            bestloss = float(f.readline())\n",
    "    else:\n",
    "        bestloss = float('inf')\n",
    "        \n",
    "    train_epoch_losses = []\n",
    "    test_epoch_losses = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        ### training ----------\n",
    "        print('\\nTraining\\n')\n",
    "        model.train()\n",
    "        train_batch_losses = []\n",
    "        with tqdm.tqdm(total=len(train_dl)) as progress_bar:\n",
    "            for x,y in train_dl:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                y_pred = model(x)\n",
    "\n",
    "                batch_size, seq_len, feats = y_pred.shape\n",
    "                y_pred_loss = y_pred.view(batch_size*seq_len,feats)\n",
    "                y_loss = y.view(-1)\n",
    "\n",
    "                loss = F.cross_entropy(y_pred_loss, y_loss, ignore_index=mask_id)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_batch_losses.append(loss.item())\n",
    "\n",
    "                progress_bar.update(1)\n",
    "        \n",
    "        avgloss = np.asarray(train_batch_losses).mean()\n",
    "        print(f\"\\nEpoch number {epoch+1} is done training. The mean average loss was {avgloss}.\\n\")\n",
    "        with open(train_losses_path, 'a') as f:\n",
    "            stringa = '\\n' + str(avgloss)\n",
    "            f.write(stringa)\n",
    "\n",
    "        ### testing ----------\n",
    "        print('\\nTesting\\n')\n",
    "        model.eval()\n",
    "        test_batch_losses = []\n",
    "        with tqdm.tqdm(total=len(test_dl)) as progress_bar:\n",
    "            for x,y in test_dl:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                y_pred = model(x)\n",
    "\n",
    "                batch_size, seq_len, feats = y_pred.shape\n",
    "                y_pred_loss = y_pred.view(batch_size*seq_len,feats)\n",
    "                y_loss = y.view(-1)\n",
    "\n",
    "                loss = F.cross_entropy(y_pred_loss, y_loss, ignore_index=mask_id)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                test_batch_losses.append(loss.item())\n",
    "\n",
    "                progress_bar.update(1)\n",
    "        \n",
    "        avgloss = np.asarray(test_batch_losses).mean()\n",
    "        print(f\"\\nEpoch number {epoch+1} is done testing. The mean average loss was {avgloss}.\\n\")\n",
    "        with open(test_losses_path, 'a') as f:\n",
    "            stringa = '\\n' + str(avgloss)\n",
    "            f.write(stringa)\n",
    "        \n",
    "        if avgloss < bestloss:\n",
    "            bestloss = avgloss\n",
    "            s = \"Loss improved! I am saving this model.\"\n",
    "            print(s)\n",
    "            logging.info(s)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            with open(bestloss_path, 'w') as f:\n",
    "                f.write(str(bestloss))\n",
    "        \n",
    "        if epoch > 0: # and epoch % 10 == 0:\n",
    "            for i in range(3):\n",
    "                print(gen_samp(model=model,vocab=vocab))\n",
    "\n",
    "        print(f\"\\nEpoch number {epoch+1} is done testing. The mean average loss was {avgloss}.\\n\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.error('something went wrong', exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3865,
     "status": "ok",
     "timestamp": 1569905899545,
     "user": {
      "displayName": "John Calabrese",
      "photoUrl": "",
      "userId": "16181148882419268806"
     },
     "user_tz": 240
    },
    "id": "iOR7cx03lGqT",
    "outputId": "132b2458-a693-4d6d-a09e-e08b87da4891"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eggs and yodt oayeelibmaesfi '"
      ]
     },
     "execution_count": 104,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "gen_samp(model,vocab, prompt='eggs and yo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1754,
     "status": "ok",
     "timestamp": 1569908747508,
     "user": {
      "displayName": "John Calabrese",
      "photoUrl": "",
      "userId": "16181148882419268806"
     },
     "user_tz": 240
    },
    "id": "-zcy3FhXqjmw",
    "outputId": "d0256eac-b67d-450b-be0d-5e19619e01fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hell4CÔ∏è\\‚Äî4 9#¬≥2√∂√©]√üÔªøqqUgM√∑RüòÆQH7 ñ}(√ú,√©be6kwv‚ÄôB])R2'I<begin>h/‚Äî+ü§î.c\n"
     ]
    }
   ],
   "source": [
    "def gen_samp_prova(model,\n",
    "            vocab,\n",
    "            prompt=''):\n",
    "    \n",
    "    seq_len = model.seq_len\n",
    "    if len(prompt) >= seq_len:\n",
    "        return prompt\n",
    "\n",
    "    mask_id = model.mask_id\n",
    "    one_hot = [mask_id for _ in range(seq_len)]\n",
    "    \n",
    "    bos = vocab.begin_idx\n",
    "    one_hot[0] = bos\n",
    "    for i,c in enumerate(prompt):\n",
    "        idx = vocab.lookup_token(c)\n",
    "        one_hot[i+1] = idx\n",
    "\n",
    "    for i in range(len(prompt), seq_len):\n",
    "        if torch.cuda.is_available():\n",
    "            hot_tensor = torch.tensor(one_hot, dtype=torch.int64, device='cuda').unsqueeze(dim=0)\n",
    "        else:\n",
    "            hot_tensor = torch.tensor(one_hot, dtype=torch.int64).unsqueeze(dim=0)\n",
    "        pred = model(hot_tensor)\n",
    "        last = pred.squeeze(dim=0)[i,:]\n",
    "        probs = F.softmax(last, dim=0)\n",
    "        winner = torch.multinomial(probs, num_samples=1)\n",
    "        one_hot[i] = winner.item()\n",
    "    \n",
    "    output = \"\"\n",
    "    for idx in one_hot:\n",
    "        token = vocab.lookup_idx(idx)\n",
    "        output += token\n",
    "    \n",
    "    start = vocab.begin_token\n",
    "    end = vocab.end_token\n",
    "    return output[output.find(start)+len(start):output.find(end)]\n",
    "\n",
    "model = Transformer(**params)\n",
    "model.to('cuda')\n",
    "print(gen_samp_prova(model,vocab, prompt='hello'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MQ8W4FPB2mW8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer_notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
