{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for google colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/johncalab/Dropbox/gitstuff/deepShowerThoughts/ai'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# # for google colab\n",
    "# os.chdir('gdrive/My Drive/ai')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class charVocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token \n",
    "                                for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "        self.mask_token = '<mask>'\n",
    "        self.begin_token = '<begin>'\n",
    "        self.end_token = '<end>'\n",
    "        self.unk_token = '<unk>'\n",
    "        self.space_token = ' '\n",
    "\n",
    "        self.mask_idx = self.add_token(self.mask_token)\n",
    "        self.begin_idx = self.add_token(self.begin_token)\n",
    "        self.end_idx = self.add_token(self.end_token)\n",
    "        self.unk_idx = self.add_token(self.unk_token)\n",
    "        self.space_idx = self.add_token(self.space_token)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.token_to_idx) == len(self.idx_to_token)\n",
    "        return len(self.token_to_idx)\n",
    "\n",
    "    def lookup_token(self,token):\n",
    "        return self.token_to_idx[token]\n",
    "\n",
    "    def lookup_idx(self,i):\n",
    "        return self.idx_to_token[i]\n",
    "\n",
    "    def add_txt(self,path):\n",
    "        with open(path, 'r') as f:\n",
    "            fulltext = f.read()\n",
    "            for c in fulltext:\n",
    "                if c != '\\n':\n",
    "                    self.add_token(c)\n",
    "        return None\n",
    "\n",
    "    def add_series(self,df):\n",
    "        for sentence in df:\n",
    "            max_len = min(300, len(sentence))\n",
    "            for char in sentence[:max_len]:\n",
    "                self.add_token(char)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class charVectorizer(object):\n",
    "    def __init__(self,vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def vectorize(self, sent, max_len=-1):\n",
    "        \"\"\"\n",
    "        max_len is used to know how much to pad\n",
    "        \"\"\"\n",
    "        ind = [self.vocab.begin_idx]\n",
    "        ind.extend(self.vocab.lookup_token(token) for token in sent)\n",
    "        ind.append(self.vocab.end_idx)\n",
    "        \n",
    "        max_len = max(len(ind), max_len)\n",
    "\n",
    "        x = np.empty(max_len-1, dtype=np.int64)\n",
    "        x[:len(ind)-1] = ind[:-1]\n",
    "        x[len(ind)-1:] = self.vocab.mask_idx\n",
    "\n",
    "        y = np.empty(max_len-1, dtype=np.int64)\n",
    "        y[:len(ind)-1] = ind[1:]\n",
    "        y[len(ind)-1:] = self.vocab.mask_idx\n",
    "\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class charDataset(Dataset):\n",
    "    def __init__(self,vectorizer,posts):\n",
    "        self.posts = posts\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "        max_len = len(posts.iloc[0])\n",
    "        for sentence in posts:\n",
    "            max_len = max(max_len, len(sentence))\n",
    "\n",
    "        self.max_len = max_len + 3\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        sent = self.posts.iloc[i]\n",
    "        x,y = self.vectorizer.vectorize(sent=sent, max_len=self.max_len)\n",
    "        assert x.shape == y.shape\n",
    "        assert x.shape[0] == self.max_len-1\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class charModel(nn.Module):\n",
    "    def __init__(self,vocab_size,\n",
    "                    embedding_dim=10,\n",
    "                    rnn_hidden_dim=9,\n",
    "                    padding_idx=0,\n",
    "                    dropout_p=0.5,\n",
    "                    num_layers=3,\n",
    "                    bidirectional=False):\n",
    "        super(charModel,self).__init__()\n",
    "\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.emb = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                embedding_dim=embedding_dim,\n",
    "                                padding_idx=padding_idx)\n",
    "\n",
    "        self.rnn = nn.GRU(input_size=embedding_dim,\n",
    "                            hidden_size=rnn_hidden_dim,\n",
    "                            dropout=dropout_p,\n",
    "                            bidirectional=bidirectional,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(in_features=2*rnn_hidden_dim,\n",
    "                out_features=vocab_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(in_features=rnn_hidden_dim,\n",
    "                out_features=vocab_size)\n",
    "\n",
    "    def forward(self, x_in, dropout=False, apply_softmax=False, verbose=False):\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Input has shape {x_in.shape}.\")\n",
    "\n",
    "        x = self.emb(x_in)\n",
    "        if verbose:\n",
    "            print(f\"Output of embedding layer has shape {x.shape}.\")\n",
    "\n",
    "        x,_ = self.rnn(x)\n",
    "        if verbose:\n",
    "            print(f\"Output of RNN has shape {x.shape}.\")\n",
    "        \n",
    "        batch_size, seq_size, _ = x.shape\n",
    "        # contiguous: pytorch requires you to reallocate memory appropriately before reshaping\n",
    "        x = x.contiguous().view(batch_size * seq_size, -1)\n",
    "        if verbose:\n",
    "            print(f\"Reshaped output of RNN has shape {x.shape}.\")\n",
    "\n",
    "        x = self.fc(x)\n",
    "        if verbose:\n",
    "            print(f\"Output of fc has shape {x.shape}.\")\n",
    "\n",
    "        if dropout:\n",
    "            x = F.dropout(x,p=self.dropout_p)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            x = F.softmax(x,dim=1)\n",
    "        \n",
    "        x = x.view(batch_size, seq_size, -1)\n",
    "        if verbose:\n",
    "            print(f\"Final output has shape {x.shape}.\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_samp(model,\n",
    "    vocab,\n",
    "    sample_size=120,\n",
    "    prompt=\"\"):\n",
    "\n",
    "    bos = vocab.begin_idx\n",
    "\n",
    "    one_hot = [bos]\n",
    "    for c in prompt:\n",
    "        idx = vocab.lookup_token(c)\n",
    "        one_hot.append(idx)\n",
    "\n",
    "    hot_tensor = torch.tensor(one_hot, dtype=torch.int64).unsqueeze(dim=0)\n",
    "    embedded = model.emb(hot_tensor)\n",
    "    _, h_n = model.rnn(embedded)\n",
    "    # h_n contains the last outputs of all layers\n",
    "    pred = model.fc(h_n[-1,:,:])\n",
    "    prob = F.softmax(pred,dim=1)\n",
    "    win = torch.multinomial(prob,num_samples=1)\n",
    "    idx=win.item()\n",
    "    one_hot.append(idx)\n",
    "\n",
    "    for i in range(100):\n",
    "        embedded = model.emb(win)\n",
    "        _, h_n = model.rnn(embedded, h_n)\n",
    "        pred = model.fc(h_n[-1,:,:])\n",
    "        prob = F.softmax(pred, dim=1)\n",
    "        win = torch.multinomial(prob,num_samples=1)\n",
    "        one_hot.append(win.item())\n",
    "\n",
    "    output = \"\"\n",
    "    for idx in one_hot:\n",
    "        token = vocab.lookup_idx(idx)\n",
    "        output += token\n",
    "\n",
    "    start = vocab.begin_token\n",
    "    end = vocab.end_token\n",
    "    return output[output.find(start)+len(start):output.find(end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'may15nov17_above130_less100_light.csv'\n",
    "csv_path = os.path.join('training_data',DATASET)\n",
    "\n",
    "rootpath = 'giacomo'\n",
    "if rootpath not in os.listdir():\n",
    "    os.mkdir(rootpath)\n",
    "\n",
    "dict_path = os.path.join(rootpath, 'dict.pkl')\n",
    "model_path = os.path.join(rootpath, 'model.pt')\n",
    "train_losses_path = os.path.join(rootpath, 'train_losses.txt')\n",
    "test_losses_path = os.path.join(rootpath, 'test_losses.txt')\n",
    "bestloss_path = os.path.join(rootpath, 'best_loss.txt')\n",
    "params_path = os.path.join(rootpath, 'params.pkl')\n",
    "\n",
    "RESUME = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv(csv_path).title.astype('U')\n",
    "\n",
    "if RESUME:\n",
    "    token_to_idx = pickle.load(open(dict_path,'rb'))\n",
    "    vocab = charVocabulary(token_to_idx=token_to_idx)\n",
    "else:\n",
    "    vocab = charVocabulary()\n",
    "    vocab.add_series(df=posts)\n",
    "    pickle.dump(vocab.token_to_idx, open(dict_path,'wb'))\n",
    "\n",
    "mask_id = vocab.mask_idx\n",
    "vectorizer = charVectorizer(vocab=vocab)\n",
    "\n",
    "full_ds = charDataset(vectorizer=vectorizer, posts=posts)\n",
    "\n",
    "if RESUME:\n",
    "    params = pickle.load(open(params_path,'rb'))\n",
    "else:\n",
    "    params = {}\n",
    "    params['vocab_size'] = len(vocab)\n",
    "    params['embedding_dim'] = 128\n",
    "    params['rnn_hidden_dim'] = 512\n",
    "    params['num_layers'] = 3\n",
    "    params['dropout_p'] = 0.5\n",
    "    params['bidirectional'] = False\n",
    "\n",
    "    pickle.dump(params, open(params_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = True\n",
    "NUM_EPOCHS = 88\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "SPLIT_FRAC = 0.20\n",
    "test_size = int(SPLIT_FRAC * len(full_ds))\n",
    "train_size = len(full_ds) - test_size\n",
    "train_ds, test_ds = random_split(full_ds, [train_size, test_size])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logpath = os.path.join(rootpath, 'logbook.log')\n",
    "logger = logging.getLogger()\n",
    "hdlr = logging.FileHandler(logpath)\n",
    "logger.addHandler(hdlr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am using cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if CUDA and torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "t_device = torch.device(device)\n",
    "\n",
    "s = f\"I am using {device}.\"\n",
    "logging.info(s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "charModel(\n",
       "  (emb): Embedding(136, 128, padding_idx=0)\n",
       "  (rnn): GRU(128, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (fc): Linear(in_features=512, out_features=136, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = charModel(**params)\n",
    "\n",
    "if RESUME:\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/267 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267/267 [06:28<00:00,  1.45s/it]\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch number 1 is done training. The mean average loss was 2.2934896338745.\n",
      "\n",
      "\n",
      "Testing\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [01:35<00:00,  1.43s/it]\n",
      "  0%|          | 0/267 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch number 1 is done testing. The mean average loss was 1.6742130660299044.\n",
      "\n",
      "Loss improved! I am saving this model.\n",
      "\n",
      "Epoch number 1 is done testing. The mean average loss was 1.6742130660299044.\n",
      "\n",
      "\n",
      "Training\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/267 [00:04<10:32,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# OK let's start training ----------------------------------------------------------\n",
    "try:\n",
    "    if RESUME:\n",
    "        with open(bestloss_path, 'r') as f:\n",
    "            bestloss = float(f.readline())\n",
    "    else:\n",
    "        bestloss = float('inf')\n",
    "        \n",
    "    train_epoch_losses = []\n",
    "    test_epoch_losses = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        ### training ----------\n",
    "        print('\\nTraining\\n')\n",
    "        model.train()\n",
    "        train_batch_losses = []\n",
    "        with tqdm.tqdm(total=len(train_dl)) as progress_bar:\n",
    "            for x,y in train_dl:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                y_pred = model(x)\n",
    "\n",
    "                batch_size, seq_len, feats = y_pred.shape\n",
    "                y_pred_loss = y_pred.view(batch_size*seq_len,feats)\n",
    "                y_loss = y.view(-1)\n",
    "\n",
    "                loss = F.cross_entropy(y_pred_loss, y_loss, ignore_index=mask_id)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_batch_losses.append(loss.item())\n",
    "\n",
    "                progress_bar.update(1)\n",
    "        \n",
    "        avgloss = np.asarray(train_batch_losses).mean()\n",
    "        print(f\"\\nEpoch number {epoch+1} is done training. The mean average loss was {avgloss}.\\n\")\n",
    "        with open(train_losses_path, 'a') as f:\n",
    "            stringa = '\\n' + str(avgloss)\n",
    "            f.write(stringa)\n",
    "\n",
    "        ### testing ----------\n",
    "        print('\\nTesting\\n')\n",
    "        model.eval()\n",
    "        test_batch_losses = []\n",
    "        with tqdm.tqdm(total=len(test_dl)) as progress_bar:\n",
    "            for x,y in test_dl:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                y_pred = model(x)\n",
    "\n",
    "                batch_size, seq_len, feats = y_pred.shape\n",
    "                y_pred_loss = y_pred.view(batch_size*seq_len,feats)\n",
    "                y_loss = y.view(-1)\n",
    "\n",
    "                loss = F.cross_entropy(y_pred_loss, y_loss, ignore_index=mask_id)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                test_batch_losses.append(loss.item())\n",
    "\n",
    "                progress_bar.update(1)\n",
    "        \n",
    "        avgloss = np.asarray(test_batch_losses).mean()\n",
    "        print(f\"\\nEpoch number {epoch+1} is done testing. The mean average loss was {avgloss}.\\n\")\n",
    "        with open(test_losses_path, 'a') as f:\n",
    "            stringa = '\\n' + str(avgloss)\n",
    "            f.write(stringa)\n",
    "        \n",
    "        if avgloss < bestloss:\n",
    "            bestloss = avgloss\n",
    "            s = \"Loss improved! I am saving this model.\"\n",
    "            print(s)\n",
    "            logging.info(s)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            with open(bestloss_path, 'w') as f:\n",
    "                f.write(str(bestloss))\n",
    "        \n",
    "        if epoch > 0: # and epoch % 10 == 0:\n",
    "            for i in range(3):\n",
    "                print(gen_samp(model=model,vocab=vocab))\n",
    "\n",
    "        print(f\"\\nEpoch number {epoch+1} is done testing. The mean average loss was {avgloss}.\\n\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.error('something went wrong', exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Conciene fish you rot\\'s fucker wheticagio and \"handiorise Forthendies I\\'ving the sux doursed to caut'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_samp(model=model,vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
